{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: I love \"machines \".\n",
      "\n",
      "Tokens: <s> i love \" machines \".\n",
      "\n",
      "Token IDs: [1, 12, 203, 21, 3760, 192]\n",
      "\n",
      "Original text: I love \"machines\".\n",
      "\n",
      "Tokens: <s> i love \" machines \".\n",
      "\n",
      "Token IDs: [1, 12, 203, 21, 3760, 192]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def analyze_tokens(text, model_name=\"qing-yao/babylm-balanced_seed-42_1e-3\"):\n",
    "    \"\"\"\n",
    "    Load a pretrained tokenizer and analyze tokens for given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "        model_name (str): Name of pretrained model from HuggingFace hub\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing tokens, token IDs, and attention mask\n",
    "    \"\"\"\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Decode each token ID to see the actual tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    \n",
    "    # Create a readable output\n",
    "    token_analysis = {\n",
    "        'original_text': text,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': encoding['input_ids'][0].tolist(),\n",
    "        'attention_mask': encoding['attention_mask'][0].tolist()\n",
    "    }\n",
    "    \n",
    "    # Print human-readable output\n",
    "    print(f\"\\nOriginal text: {text}\")\n",
    "    print(\"\\nTokens:\", \" \".join(tokens))\n",
    "    print(\"\\nToken IDs:\", encoding['input_ids'][0].tolist())\n",
    "    \n",
    "    return token_analysis\n",
    "\n",
    "# Example usage\n",
    "text = \"I love \\\"machines \\\".\"\n",
    "results = analyze_tokens(text)\n",
    "text = \"I love \\\"machines\\\".\"\n",
    "results = analyze_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qy2672/.conda/envs/qy2672/lib/python3.12/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "/home/qy2672/.conda/envs/qy2672/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound\n",
      "compound\n",
      "compound\n",
      "nsubj\n",
      "ROOT\n",
      "mark\n",
      "det\n",
      "nsubj\n",
      "ccomp\n",
      "det\n",
      "poss\n",
      "case\n",
      "dobj\n",
      "prep\n",
      "punct\n",
      "pcomp\n",
      "compound\n",
      "dobj\n",
      "prep\n",
      "pobj\n",
      "dobj\n",
      "det\n",
      "nsubj\n",
      "aux\n",
      "relcl\n",
      "prep\n",
      "pcomp\n",
      "dative\n",
      "pobj\n",
      "punct\n",
      "punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "        inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "        inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "        inf = tuple(inf)                               # Convert inf to tuple\n",
    "        infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "        infixes = [x for x in infixes if \"-|–|—|--|---|——|~\" not in x] # Remove - between letters rule\n",
    "        infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "        return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                    suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                    infix_finditer=infix_re.finditer,\n",
    "                                    token_match=nlp.tokenizer.token_match,\n",
    "                                    rules=nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "    # spacy setup (gpu is actually faster lol)\n",
    "gpu = spacy.prefer_gpu()\n",
    "print(gpu)\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "def get_children_flatten(token, depth=0, dep=False, return_tokens=False, include_self = False):\n",
    "        \"\"\"recursively get children of a given token using spacy.\"\"\"\n",
    "        children = []\n",
    "        if include_self:\n",
    "            if dep:\n",
    "                if return_tokens:\n",
    "                    children.append(\n",
    "                        (\n",
    "                            token.text.lower(),\n",
    "                            token.dep_,\n",
    "                            token.tag_,\n",
    "                            depth,\n",
    "                            token.i,\n",
    "                            token,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    children.append(\n",
    "                        (token.text.lower(), token.dep_, token.tag_, depth, token.i)\n",
    "                    )\n",
    "            else:\n",
    "                children.append(token.text.lower())\n",
    "        for child in token.children:\n",
    "            if dep:\n",
    "                if return_tokens:\n",
    "                    children.append(\n",
    "                        (\n",
    "                            child.text.lower(),\n",
    "                            child.dep_,\n",
    "                            child.tag_,\n",
    "                            depth,\n",
    "                            child.i,\n",
    "                            child,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    children.append(\n",
    "                        (child.text.lower(), child.dep_, child.tag_, depth, child.i)\n",
    "                    )\n",
    "            else:\n",
    "                children.append(child.text.lower())\n",
    "            children.extend(get_children_flatten(child, depth + 1, dep, return_tokens))\n",
    "        return children\n",
    "\n",
    "def get_phrasal_children(child):\n",
    "        children_flatten = sorted(get_children_flatten(child, dep=True, include_self=True), key=lambda x: x[4])\n",
    "        text = \"\".join([x[0] if x[0] in [\"'s\", \"`s\"] else \" \" + x[0] for x in children_flatten]).strip()\n",
    "        i = int(children_flatten[0][4])\n",
    "        return text, i\n",
    "\n",
    "sentence = \"Film critic Andrew Osmond wrote that the epilogue hurt the film's integrity for \\\"giving cartoon powers of survival that the film had rejected until then to Fritz \\\".\"\n",
    "for token in nlp(sentence):\n",
    "    print(token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qy2672",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
